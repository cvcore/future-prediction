"""This file contains discriminators for predicted frames"""
import torch
import torch.nn as nn


class DynamicsDiscriminator(nn.Module):
    """Discriminator for dynamics features.
    This module tries to discriminate between dynamics feature generated by history frames and
    dyanmics feature generated history+future frames.
    """

    def __init__(self, latent_dim, hidden_dim, n_layers):
        """
        Args:
            latent_dim (int): dimension C as in the dynamics feature [B x C x H x W]
            hidden_dim (int): intermediate dimension
            n_layers (int): number of strided convolutions. The output stride before the
                final adaptive pooling layer is then 2**n_layers
        """
        super().__init__()

        ks = 5
        strd = 2
        pd = 2
        activation = lambda: nn.LeakyReLU(0.2)
        normalization = lambda x: nn.BatchNorm2d(x)

        seq_layers = [
            nn.Conv2d(latent_dim, hidden_dim, ks, strd, pd, bias=False),
            activation(),
            # 1/2
        ]
        for l in range(1, n_layers-1):
            seq_layers.extend([
                nn.Conv2d(hidden_dim, hidden_dim * 2, ks, strd, pd, bias=False),
                normalization(hidden_dim * 2),
                activation()
            ])
            hidden_dim *= 2
        seq_layers.extend([
            nn.AdaptiveMaxPool2d(output_size=(1,1)),
            nn.Conv2d(hidden_dim, 1, 1, 1, 0)
        ])
        self.conv_layers = nn.Sequential(*seq_layers)

    def forward(self, x):
        """Forward function for dynamic discriminator.

        Args:
            x (Tensor[B, C, H, W]): dynamics feature. With perception encoder's output stride
                ranging from 8 - 32, (H, W) can vary from (32, 64) to (8, 16)

        Output:
            Tensor [B,]: probability that the input x is sampled from the future dynamics
                distribution.
        """
        x = self.conv_layers(x)
        x = x.squeeze()
        return x


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        torch.nn.init.normal_(m.weight, 0.0, 0.02)
    elif classname.find('BatchNorm') != -1:
        torch.nn.init.normal_(m.weight, 1.0, 0.02)
        torch.nn.init.zeros_(m.bias)


class SemanticsDiscriminator(nn.Module):
    """Discriminator for predicted semantic segmentations
    """

    def __init__(self, n_classes, n_layers, n_features=64):
        """
        Args:
            n_classes (int): number of semantic classes
            n_layers (int): number of strided convolution layers. In total this module will have an
                output stride of 2**n_layers before the final convolution and adaptive pooling layer.
            n_features (int): factor of number of features in this module. Defaults to 64.
        """

        super().__init__()

        make_activation = lambda: nn.ReLU()
        make_norm = lambda num_features: nn.BatchNorm2d(num_features)
        make_pool = lambda: nn.MaxPool2d(2, 2, 0)

        seq_layers = [
            nn.Conv2d(n_classes, n_features, 5, 1, 2, bias=False),
            make_activation(),
        ]

        for l in range(n_layers):
            seq_layers.extend([
                nn.Conv2d(n_features, n_features*2, 3, 1, 1, bias=False),
                make_norm(n_features*2),
                make_activation(),
                make_pool()
            ])
            n_features *= 2

        seq_layers.extend([
            nn.Conv2d(n_features, n_features*2, 3, 1, 1),
            make_norm(n_features*2),
            make_activation(),
            nn.AdaptiveMaxPool2d(1),
            nn.Conv2d(n_features*2, 1, 1, bias=True)
        ])

        self.conv_layers = nn.Sequential(*seq_layers)

        self.apply(weights_init)

    def forward(self, x, mask=None) -> torch.Tensor:
        """
        Args:
            x (Tensor[B, C, H, W] or Tensor[B, T, C, H, W]):
                expect C equals to n_classes. In case of 5D input, the first 2 dimensions will
                be first combined for the forward pass, then separated for the output.
            mask (Optional, Tensor[B, 1, H, W] or Tensor[B, T, 1, H, W]):
                if given, discriminate only the pixels with a mask value of 1. This is done by
                multiplying the mask with input 'x' before feeding it into the convolutional
                network.

        Returns:
            torch.Tensor[B] or torch.Tensor[B, T]:
                probability that the segmentation comes from groundtruth data.
        """
        in_dim = len(x.shape)
        assert in_dim == 4 or in_dim == 5

        if mask is not None:
            x = x * mask

        combine_dim = (in_dim == 5)
        if combine_dim:
            B, T, C, H, W = x.shape
            x = x.view(B*T, C, H, W)

        x = self.conv_layers(x).squeeze()

        if combine_dim:
            x = x.view(B, T)

        return x


class SemanticsVideoDiscriminator(nn.Module):
    """Discriminator for predicted semantic segmentation video
    """

    def __init__(self, n_classes):
        """
        Args:
            n_classes (int): number of semantic classes
            n_layers (int): number of strided convolution layers. In total this module will have an
                output stride of 2**n_layers before the final convolution and adaptive pooling layer.
            n_features (int): factor of number of features in this module. Defaults to 64.
        """

        super().__init__()

        def make_conv3d(
            in_channels, out_channels, kernel_size, stride,
            with_activation=True, with_batchnorm=True):

            padding = (0, kernel_size[1]//2, kernel_size[2]//2)

            has_bias = not (with_activation or with_batchnorm)
            conv_layers = [
                nn.Conv3d(in_channels, out_channels, kernel_size, stride, padding, bias=has_bias)
            ]
            if with_batchnorm:
                conv_layers.append(nn.BatchNorm3d(out_channels))
            if with_activation:
                conv_layers.append(nn.ReLU(inplace=True))

            return nn.Sequential(*conv_layers)

        self.conv_layers = nn.Sequential(                     # time dim
            make_conv3d(n_classes, 32, (5, 5, 5), (1, 2, 2)), # 15 - 4 = 11
            make_conv3d(32, 64, (3, 5, 5), (1, 2, 2)),        # 11 - 2 = 9
            make_conv3d(64, 128, (3, 3, 3), (1, 2, 2)),       #  9 - 2 = 7
            make_conv3d(128, 256, (3, 3, 3), (1, 2, 2)),      #  7 - 2 = 5
            make_conv3d(256, 512, (3, 3, 3), (1, 2, 2)),      #  5 - 2 = 3
            make_conv3d(512, 1024, (3, 3, 3), (1, 2, 2)),     #  3 - 2 = 1
            nn.AdaptiveAvgPool3d((1, 1, 1)),
            make_conv3d(1024, 1, (1, 1, 1), (1, 1, 1), False)
        )

        self.apply(weights_init)

    def forward(self, x, mask=None) -> torch.Tensor:
        """
        Args:
            x (Tensor[B, T, C, H, W]):
                semantic segmentation in video format. Expects T = 15 (5 prev + 10 pred frames)
            mask (Optional, Tensor[B, T, 1, H, W]):
                if given, discriminate only the pixels with a mask value of 1. This is done by
                multiplying the mask with input 'x' before feeding it into the convolutional
                network.

        Returns:
            torch.Tensor[B]:
                probability that the segmentation comes from groundtruth data.
        """
        in_dim = len(x.shape)
        assert in_dim == 5, "Expect video input!"
        assert x.shape[1] == 15, "Expect video have 15 frames!"

        x = x.transpose(1, 2) # B x C x T x H x W
        mask = mask.transpose(1, 2) # B x 1 x T x H x W

        if mask is not None:
            x = x * mask

        x = self.conv_layers(x).squeeze() # B

        return x


class DiscriminatorWrapper(nn.Module):
    """Wrapper class for discriminators to pack label generation with the
    `is_fake` argument in the forward pass """

    def __init__(self, discriminator, criterion):

        super().__init__()

        self.discriminator = discriminator
        self.criterion = criterion

        if isinstance(discriminator, SemanticsDiscriminator):
            self.for_video = False
        elif isinstance(discriminator, SemanticsVideoDiscriminator):
            self.for_video = True
        else:
            raise ValueError(f"Unsupported discriminator instance: {type(discriminator)}!")

    def forward(self, x, is_true, mask):
        x_shape = x.shape

        label_dims = 1 if self.for_video else 2
        if is_true:
            labels = torch.ones(x_shape[:label_dims]).to(x) # B x T
        else:
            labels = torch.zeros(x_shape[:label_dims]).to(x) # B x T

        out = self.discriminator(x, mask)
        loss = self.criterion(out, labels)

        return {'out': out, 'loss': loss}
